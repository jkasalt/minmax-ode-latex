\documentclass[main.tex]{subfiles}
\begin{document}
% Recently, there have been advances in various fields thanks to the 
% successful application of risk-minimisation machine learning. Using this
% method one aims to find a set of parameters $a_1,\dots,a_m \in \R^n$ such
% that $y = f(z; a_1,\dots, a_m)$  reaches \[ \min_y l(y) \] for some scalar field
% $l: \R^n \to \R$
In numerical optimisation, one aims to find the minimizer \[\min_x l(x)\] of a
differentiable scalar field (also known as ``loss function") $l: \R^n \to \R.$
The quintessential algorithm to solve this problem is gradient descent (GD)
which given a step size $s \in \R$ computes a sequence of iterates \[x_{k+1} = x_k + s \nabla l(x_k)\] whose
convergence properties are suitable for a wide range of applications and have been
extensively studied by thae optimization commmunity.

Nowadays, there are problems in which we would like instead to calculate
equilibria or ``saddle points" of our scalar field. We change the problem in
the following way: the loss $l: \R^n \times \R^m \to R$ now takes two arguments
$x$ and $y$ and we would like to know which are the values so that \[\min_x
	\max_y l(x,y)\] is reached.

One of the most natural approaches we could try to solve this new problem would
be to adapt gradient descent in the following way
\begin{align*}
	x_{n+1}   & = x_n - s \nabla_x l(x_n, y_n) \\
	y_{n + 1} & = y_n + s \nabla_y l(x_n, y_n)
\end{align*}
to obtain gradient descent-ascent (GDA). However we can show that even on a
simple example GDA fails to find a saddle point.
\begin{ex}
	Take $l(x,y) = xy$ with $x, y \in \R$. Then $l$ has a saddle point at the origin, but GDA gives
	\[
		\begin{bmatrix}
			x_{n+1} \\
			y_{n+1}
		\end{bmatrix}
		= \begin{bmatrix}
			1 & - s \\
			s & 1
		\end{bmatrix}
		\begin{bmatrix}
			x_n \\
			y_n
		\end{bmatrix}.
	\]
	We can see that the norm of the new iterate is $\sqrt{1 + s^2}$ times
	the old one, and so GDA diverges.
\end{ex}
For the rest of this report we write $z = (x,y)$ the concatenation of $x$ and
$y$ and $\nabla l(z) = (-\nabla_x l(x,y), \nabla_y l(x,y))$ so that GDA can be
written simply as $z_{n+1} = z_n + s \nabla l(z_n)$
Given that, many different algorithms have been proposed to find saddle points.
We give here some of the most popular ones:
\begin{itemize}
	\item (EG) Extra gradient: $z_{n+1} = z_n + s \nabla l(z_n + s \nabla
		      l(z_n))$
	\item (OGDA) Optimistic gradient descent-ascent: $z_{n+1} = z_n - 2s \nabla
		      l(z_n) + s \nabla l(z_{n-1})$
\end{itemize}
These algorithms have been shown to converge \todo{Explain in what conditions,
	cite?} and so we would like to understand if there is a fundamental difference
that causes the difference in behaviour.

\subsection{Deriving continuous time dynamics from discrete time algorithms}
Following the paper of \cite{suDifferentialEquationModeling2016}, there has
been a lot of attention towards deriving differential equations $\dot Z(t) =
	f(Z(t), t)$ whose trajectory follows a given discrete time algorithms. In this
subsection we show that by simply letting the timestep $s \to 0$ all of the
algorithms presented so far yield the same ODE, gradient flow (GF). The main
idea is to assume that the discrete algorithm is obtained from a continuous
curve at fixed length times \[z_n = Z(t_n)\] where $t_n = t_0 + ns.$ Using
this, algorithms of the form \[z_{n+1} = z_n + s\varphi(z_n)\] can be rewritten
as \[\frac{Z(t_n + s) - Z(t_n)}{s} = \varphi(Z(t_n)).\] Finally, taking limits
$s \to 0$ gives \[\dot Z(t) = \varphi(Z(t)),\] where we dropped the index from
$t_n.$
\begin{ex}
	Given that in GDA $\varphi(z_n)$ takes the form $\nabla l(z_n)$ we obtain a
	continuous time approximation \[\dot Z(t) = \nabla l(Z(t)),\] which we call gradient flow.
\end{ex}
\begin{ex}
	For EG, the update step gives \[\frac{Z(t + s) - Z(t)}{s} = \nabla l
		\big(Z(t) + s \nabla l(Z(t))\big),\]
	and taking the limits gives gradient flow again.
\end{ex}
\todo{todo: OGDA}  Given that GDA diverges even in simple cases and that the
other two algorithms are known to converge in a large class of situations, we
are quite dissatisfied in doscovering that they present the same continuous
time dynamics.
\todo{todo: natural descent from mirror descent}
In Chapter~\ref{chap:osr} we will present a new method that makes use of
modified differential equations to obtain continuous time approximations that
more closely follow their discrete time counterparts.

